"""
RedMesh v1 main API endpoints plugin - for managed execution of decentralized distributed pentesting jobs 
at scale in the Ratio1.ai network

- heterogenous worker orchestration
- dynamic job scheduling
- plugin-based scanning and testing services
- real-time job monitoring and logging via API

"""


from naeural_core.business.default.web_app.fast_api_web_app import FastApiWebAppPlugin as BasePlugin
from .redmesh_utils import PentestLocalWorker  # Import PentestJob from separate module

__VER__ = '0.7.5'  # updated version

_CONFIG = {
  **BasePlugin.CONFIG,
  
  'PORT': None,
  
  "CHECK_JOBS_EACH" : 5,
  
  "NR_LOCAL_WORKERS" : 8,
  
  "WARMUP_DELAY" : 30,
  
  
  'VALIDATION_RULES': {
    **BasePlugin.CONFIG['VALIDATION_RULES'],  
  },
}

class PentesterApi01Plugin(BasePlugin):
  """
  RedMesh API - a pentesting meta-plugin for receiving pentesting targets and performing operations.
  Supports asynchronous job execution and performs distributed red-team attacks based on 
  decentralized workers orchestrated using CStore.
  """
  CONFIG = _CONFIG


  def on_init(self):
    super(PentesterApi01Plugin, self).on_init()
    self.__features = self._get_all_features()
    # Track active and completed jobs by target
    self.scan_jobs = {}       # target -> PentestJob instance
    self.completed_jobs_reports = {}  # target -> final report dict
    self.lst_completed_jobs = []  # List of completed jobs
    self.__last_checked_jobs = 0
    self.__warmupstart = self.time()
    self.__warmup_done = False
    current_epoch = self.netmon.epoch_manager.get_current_epoch()
    self.P("Started {} plugin in epoch {}. Current features:\n{}".format(
      self.__class__.__name__, current_epoch, 
      self.json_dumps(self.__features, indent=2),
    ))
    return
  
  def P(self, s, *args, **kwargs):
    s = "[REDMESH] " + s
    return super(PentesterApi01Plugin, self).P(s, *args, **kwargs)
  
  
  def __post_init(self):
    all_network_jobs = self.chainstore_hgetall(hkey=self.cfg_instance_id)
    info = ""
    for target, job_spec in all_network_jobs.items():
      if not isinstance(job_spec, dict):
        info += f"- [INVALID]: {target}: {job_spec}\n"
      else:
        report = job_spec.get("workers", {}).get(self.ee_addr, {}).get("result",{})
        if "initiator" not in report and len(report) > 0:
          self.P(f"Found incomplete report for {target}, aggregating...", color='r')
          agg_report = self._get_aggregated_report(report)
          job_spec["workers"][self.ee_addr]["result"] = agg_report
          self.chainstore_hset(hkey=self.cfg_instance_id, key=self.ee_addr, value=job_spec)
          # aggregate this report and repost it
        info += f"- [VALID]:   {target}: {self.json_dumps(job_spec, indent=2)}\n"
    self.P("Warmup complete. Current jobs for {}:\n{}".format(
      self.cfg_instance_id, 
      info
    ))
    self.__warmup_done = True
    return



  def _get_all_features(self, categs=False):
    features = {} if categs else []
    PREFIXES = ["_service_info_", "_web_test_"]
    for prefix in PREFIXES:
      methods = [method for method in dir(PentestLocalWorker) if method.startswith(prefix)]
      if categs:
        features[prefix[1:-1]] = methods
      else:
        features.extend(methods)
    return features


  def _maybe_launch_jobs(self, nr_local_workers=None):
    """
    Launch new PentestJob threads for any announced pentest target.
    Called at each process iteration.
    """
    if self.time() - self.__last_checked_jobs > self.cfg_check_jobs_each:      
      self.__last_checked_jobs = self.time()            
      all_jobs = self.chainstore_hgetall(hkey=self.cfg_instance_id)
      for network_worker_address, job_specs in all_jobs.items():
        if not isinstance(job_specs, dict):
          continue
        target = job_specs.get("target")
        job_id = job_specs.get("job_id", target).replace(".", "")
        workers = job_specs.get("workers", {}) # get network workers
        current_worker_finished = workers.get(self.ee_addr, {}).get("finished", False)
        if job_id is None or current_worker_finished:
          continue  
        # If job not already running and not completed, start a new thread
        closed_target = job_id in self.completed_jobs_reports
        in_progress_target = job_id in self.scan_jobs        
        if not in_progress_target and not closed_target:
          self.P("Using {} local workers for job {}".format(nr_local_workers, job_id))
          start_port = job_specs.get("start_port")
          if start_port is None:
            self.P("No start port specified, defaulting to 1.")
            start_port = 1
          end_port = job_specs.get("end_port")
          if end_port is None:
            self.P("No end port specified, defaulting to 65535.")
            end_port = 65535
          exceptions = job_specs.get("exceptions", [])
          if nr_local_workers is None:
            nr_local_workers = self.cfg_nr_local_workers
          ports = list(range(start_port, end_port + 1))
          batches = []
          ports = sorted(ports)
          nr_ports = len(ports)
          for i in range(nr_local_workers):
            start = i * (nr_ports // nr_local_workers)
            end = (i + 1) * (nr_ports // nr_local_workers)
            batch = ports[start:end]
            batches.append(batch)
          #endfor create batches
          if job_id not in self.scan_jobs:
            self.scan_jobs[job_id] = {}
          for batch in batches:
            self.P("Launching {} requested by {} for target {} - {} ports [{}-{}]".format(
              job_id, network_worker_address,
              target, len(batch), min(batch), max(batch)
            ))
            batch_job = PentestLocalWorker(
              owner=self,
              target=target, 
              job_id=job_id,
              initiator=network_worker_address,
              exceptions=exceptions,
              worker_target_ports=batch,
            )
            self.scan_jobs[job_id][batch_job.local_worker_id] = batch_job
            batch_job.start()
          #end for each batch launch a PentestLocalWorker
        #endif
      #enfor
    #endif
    return
  
  
  def _get_aggregated_report(self, local_jobs):
    dct_aggregated_report = {}
    if local_jobs:
      self.P(f"Aggregating reports from {len(local_jobs)} local jobs...")
      for local_worker_id, local_job_status in local_jobs.items():
        aggregation_fields = PentestLocalWorker.get_worker_specific_result_fields()
        for field in local_job_status:          
          if field not in dct_aggregated_report:
            dct_aggregated_report[field] = local_job_status[field]
          elif field in aggregation_fields:
            if field not in dct_aggregated_report:
              field_type = type(local_job_status[field])
              dct_aggregated_report[field] = field_type()
            if isinstance(dct_aggregated_report[field], list):
              existing = set(dct_aggregated_report[field])
              dct_aggregated_report[field] = existing.union(local_job_status[field])
            elif isinstance(dct_aggregated_report[field], dict):
              dct_aggregated_report[field] = {
                **dct_aggregated_report[field], 
                **local_job_status[field]
              }
            else:
              raise ValueError(f"Cannot aggregate field {field} of type {type(dct_aggregated_report[field])}")
            # end if aggregation type
          # end if standard (one time) or aggregated fields
        # for each field in this local job
      # for each local job
      self.P(f"Report aggregation done.")
    # endif we have local jobs
    return dct_aggregated_report


  def _close_job(self, job_id, canceled=False):
    """
    This only closes the LOCAL job not the whole network job - must be refactored!
    
    TODO: change the logic as follows
    - separate worker status from job status in different hset
      - redmesh instance (network) cfg_instance_id
      - job-id hset
    - each network worker will post status (for its local workers)
    - all network workers will monitor all network workers
    - last network worker that finishes posts 
    
    """
    local_workers = self.scan_jobs.pop(job_id, None)
    if local_workers:
      local_reports = {
        local_worker_id: local_worker.get_status()
        for local_worker_id, local_worker in local_workers.items()
      }
      report = self._get_aggregated_report(local_reports)
      if report:
        job_specs = self.chainstore_hget(hkey=self.cfg_instance_id, key=self.ee_addr)
        closing = "Forced" if canceled else "Post finish"
        job_specs["workers"][self.ee_addr]["finished"] = True
        job_specs["workers"][self.ee_addr]["result"] = report
        job_specs["workers"][self.ee_addr]["canceled"] = canceled
        self.P("{} closing job_id {}:\n{}".format(
          closing,
          job_id,
          self.json_dumps(job_specs, indent=2)
        ))
        self.chainstore_hset(hkey=self.cfg_instance_id, key=self.ee_addr, value=job_specs)
    return


  def _maybe_close_jobs(self):
    for job_id, local_workers in list(self.scan_jobs.items()):
      all_workers_done = True
      any_canceled_worker = False
      reports = {}
      job : PentestLocalWorker = None
      initiator = None
      for local_worker_id, job in local_workers.items():
        # If thread finished or job flagged as done, collect result
        if not job.thread.is_alive() or job.state.get("done"):
          if job_id not in self.completed_jobs_reports:
            self.completed_jobs_reports[job_id] = {}
          # Prepare final report for this job
          local_worker_report = job.get_status()
          any_canceled_worker = any_canceled_worker or local_worker_report.get("canceled", False)
          # Save completed report
          self.completed_jobs_reports[job_id][local_worker_id] = local_worker_report
          reports[local_worker_id] = local_worker_report
          self.P("Worker {} has finished job_id {} for target {}:\n{}".format(
            local_worker_id, job_id, local_worker_report['target'],
            self.json_dumps(local_worker_report, indent=2)
          ))
        else:
          all_workers_done = False
        initiator = job.initiator
      #end for each worker
      
      if all_workers_done:
        self.lst_completed_jobs.append(job_id)
        self.P(f"All local workers for job {job_id} from initiator {initiator} have finished.")
        try:
          self._close_job(job_id, canceled=any_canceled_worker)
        except Exception as e:
          self.P(f"Error clearing job from chainstore: {e}")
          # Remove from active jobs
          del self.scan_jobs[job_id]
        #end if
      #end for each local worker of a job
    #end for each job
    return


  def _get_all_network_jobs(self):
    all_workers_and_jobs = self.chainstore_hgetall(hkey=self.cfg_instance_id)
    return all_workers_and_jobs


  def _get_job_from_cstore(self, job_id: str):
    all_workers_and_jobs = self._get_all_network_jobs()
    found = None
    for network_worker in all_workers_and_jobs:
      job_specs = all_workers_and_jobs.get(network_worker)
      if isinstance(job_specs, dict) and job_specs.get("job_id") == job_id:
        found = job_specs
        break
    return found


  def _get_job_status(self, job_id : str):
    target = None
    local_workers = self.scan_jobs.get(job_id)
    jobs_network_state = self._get_job_from_cstore(job_id)
    result = {}
    # first check if in completed jobs
    if job_id in self.lst_completed_jobs: 
      # dont check in the reports that might contain only from some local workers
      local_workers_reports = self.completed_jobs_reports[job_id]
      some_worker = list(local_workers_reports.keys())[0]
      target = local_workers_reports[some_worker]["target"]
      result = {
        "job_id": job_id,
        "target": target,
        "status": "completed",
        "report": self.completed_jobs_reports[job_id]
      }
    
    elif local_workers:
      # If job is currently running, return progress info   
      statuses = {}
      for local_worker_id, job in local_workers.items():
        statuses[local_worker_id] = job.get_status()
      #end for each local worker
      result = statuses
    # Job not found
    else:
      # TODO: check job in cstore maybe it was finished some time 
      #       ago before current deployment
      result = {
        "job_id": job_id,
        "target": target,
        "status": "not_found",
        "message": "No such job is running or has been completed on this worker."
      }
    #endif
    return result


  """
  
  Endpoints:
  
  """

  @BasePlugin.endpoint
  def list_features(self):
    result = {"features": self._get_all_features(categs=True)}
    return result


  @BasePlugin.endpoint
  def launch_test(
    self, 
    target: str = "", 
    start_port: int = 1, end_port: int = 65535, 
    exceptions: str = "64297"
  ):
    """
    Endpoint to start a pentest on the specified target.
    Announce job to network via CStore and return current jobs.            
    """
    # INFO: This method only announces the job to the network. It does not 
    #       execute the job itself - that part is handled by PentestJob
    #       executed after periodical check from plugin process.
    if not target:
      raise ValueError("No target specified.")

    start_port = int(start_port)
    end_port = int(end_port)

    if len(exceptions) > 0:
      exceptions = [
        int(x) for x in self.re.findall(r'\d+', exceptions)
        if x.isdigit()
    ]
    job_id = self.uuid(8)
    self.P(f"Launching {job_id=} {target=} with {exceptions=}")
    self.P(f"Announcing pentest to workers (instance_id {self.cfg_instance_id})...")
    job_specs = {
      "job_id" : job_id,
      "target": target,
      "exceptions" : exceptions,
      "start_port" : start_port,
      "end_port" : end_port,
      "workers" : {
        self.ee_addr: {
          "finished": False,
          "result": None
        }
      },
    }
    self.chainstore_hset(
      hkey=self.cfg_instance_id,
      key=self.ee_addr,
      value=job_specs
    )
    all_network_jobs = self.chainstore_hgetall(hkey=self.cfg_instance_id)
    report = {}
    for launcher, job_specs in all_network_jobs.items():
      is_current_job = isinstance(job_specs, dict) and job_specs.get("target") == target and launcher == self.ee_addr
      if not is_current_job:
        report[launcher] = job_specs
    #end for
    
    self.P(f"Current jobs:\n{self.json_dumps(all_network_jobs, indent=2)}")
    result = {
      "job_specs": job_specs,
      "worker": self.ee_addr,
      "other_jobs": report,
    }
    return result


  @BasePlugin.endpoint
  def get_job_status(self, job_id: str):
    """
    Endpoint to retrieve the status or final report of a pentest job for the given target.
    
    TODO: Data must be extracted from CStore
    """
    # If job has completed, return its report
    return self._get_job_status(job_id)


  @BasePlugin.endpoint
  def list_network_jobs(self):
    """
    Endpoint to list all network jobs.
    """
    all_network_jobs = self.chainstore_hgetall(hkey=self.cfg_instance_id)
    return all_network_jobs
  
  
  @BasePlugin.endpoint
  def list_local_jobs(self):
    jobs = {
      job_id: self._get_job_status(job_id)
      for job_id, local_workers in self.scan_jobs.items()
    }
    return jobs
  
  
  @BasePlugin.endpoint
  def stop_and_delete_job(self, job_id : str):
    """
    Endpoint to stop and delete a pentest job.
    """
    # Stop the job if it's running
    local_workers = self.scan_jobs.get(job_id)
    if local_workers:
      self.P(f"Stopping and deleting job {job_id}.")
      for local_worker_id, job in local_workers.items():
        self.P(f"Stopping job {job_id} on local worker {local_worker_id}.")
        job.stop()
      self.P(f"Job {job_id} stopped.")
    # Remove from active jobs
    self.scan_jobs.pop(job_id, None)
    self.P(f"Job {job_id} deleted.")
    return {"status": "success", "job_id": job_id}


  def process(self):
    """
    Periodically invoked to manage job threads.
    Launches new jobs and checks for completed ones.
    """
    super(PentesterApi01Plugin, self).process()
    
    if (self.time() - self.__warmupstart) < self.cfg_warmup_delay:
      # we do not start jobs before API warmup
      return
    elif not self.__warmup_done:
      self.__post_init()
    #endif 
    # Launch any new jobs
    self._maybe_launch_jobs()
    # Check active jobs for completion
    self._maybe_close_jobs()
    return