"""
RedMesh v1 main API endpoints plugin - for managed execution of decentralized distributed pentesting jobs 
at scale in the Ratio1.ai network

- heterogenous worker orchestration
- dynamic job scheduling
- plugin-based scanning and testing services
- real-time job monitoring and logging via API

"""


from naeural_core.business.default.web_app.fast_api_web_app import FastApiWebAppPlugin as BasePlugin
from .redmesh_utils import PentestLocalWorker  # Import PentestJob from separate module

__VER__ = '0.8.1'  # updated version

_CONFIG = {
  **BasePlugin.CONFIG,
  
  'PORT': None,
  
  "CHECK_JOBS_EACH" : 5,
  
  "NR_LOCAL_WORKERS" : 8,
  
  "WARMUP_DELAY" : 30,
  
  
  'VALIDATION_RULES': {
    **BasePlugin.CONFIG['VALIDATION_RULES'],  
  },
}

class PentesterApi01Plugin(BasePlugin):
  """
  RedMesh API - a pentesting meta-plugin for receiving pentesting targets and performing operations.
  Supports asynchronous job execution and performs distributed red-team attacks based on 
  decentralized workers orchestrated using CStore.

  Supports semaphore-based pairing with Container App Runner plugins via
  the SEMAPHORE configuration key. When configured, exposes API host/port
  as environment variables to paired containers (e.g., RedMesh UI).
  """
  CONFIG = _CONFIG


  def on_init(self):
    super(PentesterApi01Plugin, self).on_init()
    self.__features = self._get_all_features()
    # Track active and completed jobs by target
    self.scan_jobs = {}       # target -> PentestJob instance
    self.completed_jobs_reports = {}  # target -> final report dict
    self.lst_completed_jobs = []  # List of completed jobs
    self.__last_checked_jobs = 0
    self.__warmupstart = self.time()
    self.__warmup_done = False
    current_epoch = self.netmon.epoch_manager.get_current_epoch()
    self.P("Started {} plugin in epoch {}. Current features:\n{}".format(
      self.__class__.__name__, current_epoch,
      self.json_dumps(self.__features, indent=2),
    ))
    return


  def _setup_semaphore_env(self):
    """Set semaphore environment variables for paired plugins."""
    localhost_ip = self.log.get_localhost_ip()
    port = self.cfg_port
    self.semaphore_set_env('API_HOST', localhost_ip)
    if port:
      self.semaphore_set_env('API_PORT', str(port))
      self.semaphore_set_env('API_URL', 'http://{}:{}'.format(localhost_ip, port))
    return


  def on_close(self):
    super(PentesterApi01Plugin, self).on_close()
    return


  def P(self, s, *args, **kwargs):
    s = "[REDMESH] " + s
    return super(PentesterApi01Plugin, self).P(s, *args, **kwargs)
  
  
  def __post_init(self):
    all_network_jobs = self.chainstore_hgetall(hkey=self.cfg_instance_id)
    info = ""
    for job_key, job_spec in all_network_jobs.items():
      try:
        normalized_key, normalized_spec = self._normalize_job_record(job_key, job_spec, migrate=True)
        if normalized_key is None:
          info += f"- [INVALID]: from <{job_key}>:\n{job_spec}\n"
          continue
        our_worker = normalized_spec.get("workers", {}).get(self.ee_addr, {})
        raw_report = our_worker.get("result", {}) if isinstance(our_worker, dict) else {}
        needs_aggregation = False
        if isinstance(raw_report, dict):
          sample_value = next(iter(raw_report.values()), None)
          needs_aggregation = isinstance(sample_value, dict) and "local_worker_id" in sample_value
        if needs_aggregation:
          self.P(f"Found incomplete report for {normalized_key}, aggregating...", color='r')
          agg_report = self._get_aggregated_report(raw_report)
          our_worker["result"] = agg_report
          normalized_spec["workers"][self.ee_addr] = our_worker
          self.chainstore_hset(hkey=self.cfg_instance_id, key=normalized_key, value=normalized_spec)
        is_completed = all(
          worker.get("finished") for worker in normalized_spec.get("workers", {}).values()
        ) if normalized_spec.get("workers") else False
        info += (
          f"- [VALID][{'COMPLETED' if is_completed else 'INCOMPLETE'}]"
          f" job <{normalized_key}> from <{normalized_spec.get('launcher')}>:\n"
          f"{self.json_dumps(normalized_spec, indent=2)}\n"
        )
      except Exception as e:
        self.P("Error processing job spec for {}: {}\n{}\n{}".format(
            job_key, job_spec, e, self.trace_info(),
            self.json_dumps(job_spec, indent=2)
          ), color='r'
        )
      #end try
    #end for each job spec
    self.P("RedMesh instance <{}> warmup complete. Monitoring jobs...".format(
        self.cfg_instance_id
      ), 
      boxed=True
    )
    self.P("Current jobs in decentralized instances:\n{}".format(
      info
    ))
    self.__warmup_done = True
    return



  def _get_all_features(self, categs=False):
    features = {} if categs else []
    PREFIXES = ["_service_info_", "_web_test_"]
    for prefix in PREFIXES:
      methods = [method for method in dir(PentestLocalWorker) if method.startswith(prefix)]
      if categs:
        features[prefix[1:-1]] = methods
      else:
        features.extend(methods)
    return features


  def _normalize_job_record(self, job_key, job_spec, migrate=False):
    """Return a normalized job record and optionally migrate legacy entries.

    Stage 1 of the multi-job coordination refactor lives here. The helper keeps
    legacy single-key entries compatible while allowing each job to live under
    its own hash key. Stage 2 (not yet implemented) will introduce a dedicated
    index for fast lookups and TTL-based cleanup so workers can garbage collect
    stale jobs without clobbering peers.
    """
    if not isinstance(job_spec, dict):
      return None, None
    normalized = dict(job_spec)
    job_id = normalized.get("job_id") or job_key
    normalized["job_id"] = job_id
    launcher = normalized.get("launcher") or normalized.get("initiator") or job_key
    normalized["launcher"] = launcher
    workers = normalized.get("workers")
    if not isinstance(workers, dict):
      workers = {}
    normalized["workers"] = workers
    if migrate and job_key != job_id:
      self.chainstore_hset(hkey=self.cfg_instance_id, key=job_id, value=normalized)
      self.chainstore_hset(hkey=self.cfg_instance_id, key=job_key, value=None)
      job_key = job_id
    return job_key, normalized


  def _ensure_worker_entry(self, job_id, job_spec):
    workers = job_spec.setdefault("workers", {})
    worker_entry = workers.get(self.ee_addr)
    if worker_entry is None:
      worker_entry = {"finished": False, "result": None}
      workers[self.ee_addr] = worker_entry
      self.chainstore_hset(hkey=self.cfg_instance_id, key=job_id, value=job_spec)
    return worker_entry
  
  
  def _launch_job(
    self, 
    job_id,
    target,
    start_port, 
    end_port, 
    network_worker_address,
    nr_local_workers=4,
    exceptions=None,
  ):
    local_jobs = {}
    ports = list(range(start_port, end_port + 1))
    batches = []
    ports = sorted(ports)
    nr_ports = len(ports)
    if nr_ports == 0:
      raise ValueError("No ports available for local workers.")
    nr_local_workers = max(1, min(nr_local_workers, nr_ports))
    base_chunk, remainder = divmod(nr_ports, nr_local_workers)
    start = 0
    if exceptions is None:
      exceptions = []
    for i in range(nr_local_workers):
      chunk = base_chunk + (1 if i < remainder else 0)
      end = start + chunk
      batch = ports[start:end]
      if batch:
        batches.append(batch)
      start = end
    #endfor create batches
    if not batches:
      raise ValueError("Unable to allocate port batches to workers.")
    if job_id not in self.scan_jobs:
      self.scan_jobs[job_id] = {}
    for i, batch in enumerate(batches):
      try:
        start_port = batch[0]
        end_port = batch[-1]
        self.P("Launching {} requested by {} for target {} - {} ports [{}-{}]".format(
          job_id, network_worker_address,
          target, len(batch), start_port, end_port
        ))
        batch_job = PentestLocalWorker(
          owner=self,
          local_id_prefix=str(i + 1),
          target=target, 
          job_id=job_id,
          initiator=network_worker_address,
          exceptions=exceptions,
          worker_target_ports=batch,
        )
        batch_job.start()
        local_jobs[batch_job.local_worker_id] = batch_job
      except Exception as exc:
        self.P(
          "Failed to launch batch local job for ports [{}-{}]: {}".format(
            batch[0] if batch else "-",
            batch[-1] if batch else "-",
            exc
          ),
          color='r'
        )
    #end for each batch launch a PentestLocalWorker
    if not local_jobs:
      raise ValueError("No local workers could be launched for the requested port range.")
    return local_jobs

  def _maybe_launch_jobs(self, nr_local_workers=None):
    """
    Launch new PentestJob threads for any announced pentest target.
    Called at each process iteratisson.
    """
    if self.time() - self.__last_checked_jobs > self.cfg_check_jobs_each:      
      self.__last_checked_jobs = self.time()            
      all_jobs = self.chainstore_hgetall(hkey=self.cfg_instance_id)
      for job_key, job_specs in all_jobs.items():
        normalized_key, job_specs = self._normalize_job_record(job_key, job_specs, migrate=True)
        if normalized_key is None:
          continue  
        target = job_specs.get("target")
        job_id = job_specs.get("job_id", normalized_key)
        if job_id is None:
          continue
        worker_entry = self._ensure_worker_entry(job_id, job_specs)
        current_worker_finished = worker_entry.get("finished", False)
        if current_worker_finished:
          continue
        # If job not already running and not completed, start a new thread
        closed_target = job_id in self.completed_jobs_reports
        in_progress_target = job_id in self.scan_jobs        
        if not in_progress_target and not closed_target:
          launcher = job_specs.get("launcher")
          self.P(f"Starting job {job_id}, target {target} from {launcher}", boxed=True)
          start_port = job_specs.get("start_port")
          if start_port is None:
            self.P("No start port specified, defaulting to 1.")
            start_port = 1
          end_port = job_specs.get("end_port")
          if end_port is None:
            self.P("No end port specified, defaulting to 65535.")
            end_port = 65535
          exceptions = job_specs.get("exceptions", [])
          workers_requested = nr_local_workers if nr_local_workers is not None else self.cfg_nr_local_workers
          self.P("Using {} local workers for job {}".format(workers_requested, job_id))
          try:
            local_jobs = self._launch_job(
              job_id=job_id,
              target=target,
              start_port=start_port,
              end_port=end_port,
              network_worker_address=launcher,
              nr_local_workers=workers_requested,
              exceptions=exceptions
            )
          except ValueError as exc:
            self.P(f"Skipping job {job_id}: {exc}", color='r')
            worker_entry["finished"] = True
            worker_entry["error"] = str(exc)
            self.chainstore_hset(hkey=self.cfg_instance_id, key=job_id, value=job_specs)
            continue
          self.scan_jobs[job_id] = local_jobs
        #endif need to launch new job
      #end for each potential new job
    #endif it is time to check
    return
  
  
  def _get_aggregated_report(self, local_jobs):
    dct_aggregated_report = {}
    type_or_func, field = None, None
    try:
      if local_jobs:
        self.P(f"Aggregating reports from {len(local_jobs)} local jobs...")
        for local_worker_id, local_job_status in local_jobs.items():
          aggregation_fields = PentestLocalWorker.get_worker_specific_result_fields()
          for field in local_job_status:          
            if field not in dct_aggregated_report:
              dct_aggregated_report[field] = local_job_status[field]
            elif field in aggregation_fields:
              type_or_func = aggregation_fields[field]
              if field not in dct_aggregated_report:
                field_type = type(local_job_status[field])
                dct_aggregated_report[field] = field_type()
              #endif
              if isinstance(dct_aggregated_report[field], list):
                existing = set(dct_aggregated_report[field])
                merged = existing.union(local_job_status[field])
                try:
                  dct_aggregated_report[field] = sorted(merged)
                except TypeError:
                  dct_aggregated_report[field] = list(merged)
              elif isinstance(dct_aggregated_report[field], dict):
                dct_aggregated_report[field] = {
                  **dct_aggregated_report[field], 
                  **local_job_status[field]
                }
              else:
                _existing = dct_aggregated_report[field]
                _new = local_job_status[field]
                dct_aggregated_report[field] = type_or_func([_existing, _new])
              # end if aggregation type
            # end if standard (one time) or aggregated fields
          # for each field in this local job
        # for each local job
        self.P(f"Report aggregation done.")
      # endif we have local jobs
    except Exception as exc:
      self.P("Error during report aggregation: {}:\n{}\n{}\ntype_or_func={}, field={}".format(
        exc, self.trace_info(),
        self.json_dumps(dct_aggregated_report, indent=2),
        type_or_func, field
      ))
    return dct_aggregated_report


  def _close_job(self, job_id, canceled=False):
    """
    This only closes the LOCAL job not the whole network job - must be refactored!
    
    TODO: change the logic as follows
    - separate worker status from job status in different hset
      - redmesh instance (network) cfg_instance_id
      - job-id hset
    - each network worker will post status (for its local workers)
    - all network workers will monitor all network workers
    - last network worker that finishes posts 
    
    """
    local_workers = self.scan_jobs.pop(job_id, None)
    if local_workers:
      local_reports = {
        local_worker_id: local_worker.get_status()
        for local_worker_id, local_worker in local_workers.items()
      }
      report = self._get_aggregated_report(local_reports)
      if report:
        raw_job_specs = self.chainstore_hget(hkey=self.cfg_instance_id, key=job_id)
        if raw_job_specs is None:
          self.P(f"Job {job_id} no longer present in chainstore; skipping close sync.", color='r')
          return
        _, job_specs = self._normalize_job_record(job_id, raw_job_specs)
        closing = "Forced" if canceled else "Post finish"
        worker_entry = job_specs.setdefault("workers", {}).setdefault(self.ee_addr, {})
        worker_entry["finished"] = True
        worker_entry["result"] = report
        worker_entry["canceled"] = canceled
        job_specs["workers"][self.ee_addr] = worker_entry
        self.P("{} closing job_id {}:\n{}".format(
          closing,
          job_id,
          self.json_dumps(job_specs, indent=2)
        ))
        self.chainstore_hset(hkey=self.cfg_instance_id, key=job_id, value=job_specs)
    return


  def _maybe_close_jobs(self):
    for job_id, local_workers in list(self.scan_jobs.items()):
      all_workers_done = True
      any_canceled_worker = False
      reports = {}
      job : PentestLocalWorker = None
      initiator = None
      nr_local_workers = len(local_workers)
      for local_worker_id, job in local_workers.items():
        # If thread finished or job flagged as done, collect result
        if not job.thread.is_alive() or job.state.get("done"):
          if job_id not in self.completed_jobs_reports:
            self.completed_jobs_reports[job_id] = {}
          # Prepare final report for this job
          local_worker_report = job.get_status()
          any_canceled_worker = any_canceled_worker or local_worker_report.get("canceled", False)
          # Save completed report
          self.completed_jobs_reports[job_id][local_worker_id] = local_worker_report
          reports[local_worker_id] = local_worker_report
          self.P("Worker {} has finished job_id {} for target {}:\n{}".format(
            local_worker_id, job_id, local_worker_report['target'],
            self.json_dumps(local_worker_report, indent=2)
          ))
        else:
          all_workers_done = False
        initiator = job.initiator
      #end for each worker
      
      if all_workers_done:
        self.lst_completed_jobs.append(job_id)
        self.P(f"All {nr_local_workers} local workers for job {job_id} from initiator {initiator} have finished.")
        try:
          self._close_job(job_id, canceled=any_canceled_worker)
        except Exception as e:
          self.P(f"Error clearing job from chainstore: {e}")
          # Remove from active jobs
          del self.scan_jobs[job_id]
        #end if
      #end for each local worker of a job
    #end for each job
    return


  def _get_all_network_jobs(self):
    all_workers_and_jobs = self.chainstore_hgetall(hkey=self.cfg_instance_id)
    return all_workers_and_jobs


  def _get_job_from_cstore(self, job_id: str):
    all_workers_and_jobs = self._get_all_network_jobs()
    found = None
    for job_key, job_specs in all_workers_and_jobs.items():
      _, normalized = self._normalize_job_record(job_key, job_specs)
      if normalized and normalized.get("job_id") == job_id:
        found = normalized
        break
    return found


  def _get_job_status(self, job_id : str):
    target = None
    local_workers = self.scan_jobs.get(job_id)
    jobs_network_state = self._get_job_from_cstore(job_id)
    result = {}
    # first check if in completed jobs
    if job_id in self.lst_completed_jobs: 
      # dont check in the reports that might contain only from some local workers
      local_workers_reports = self.completed_jobs_reports[job_id]
      some_worker = list(local_workers_reports.keys())[0]
      target = local_workers_reports[some_worker]["target"]
      result = {
        "job_id": job_id,
        "target": target,
        "status": "completed",
        "report": self.completed_jobs_reports[job_id]
      }
    
    elif local_workers:
      # If job is currently running, return progress info   
      statuses = {}
      for local_worker_id, job in local_workers.items():
        statuses[local_worker_id] = job.get_status()
      #end for each local worker
      result = statuses
    elif jobs_network_state:
      result = {
        "job_id": job_id,
        "target": jobs_network_state.get("target"),
        "status": "network_tracked",
        "job": jobs_network_state
      }
    # Job not found
    else:
      # TODO: check job in cstore maybe it was finished some time 
      #       ago before current deployment
      result = {
        "job_id": job_id,
        "target": target,
        "status": "not_found",
        "message": "No such job is running or has been completed on this worker."
      }
    #endif
    return result


  """
  
  Endpoints:
  
  """

  @BasePlugin.endpoint
  def list_features(self):
    result = {"features": self._get_all_features(categs=True)}
    return result


  @BasePlugin.endpoint
  def launch_test(
    self, 
    target: str = "", 
    start_port: int = 1, end_port: int = 65535, 
    exceptions: str = "64297"
  ):
    """
    Endpoint to start a pentest on the specified target.
    Announce job to network via CStore and return current jobs.            
    """
    # INFO: This method only announces the job to the network. It does not 
    #       execute the job itself - that part is handled by PentestJob
    #       executed after periodical check from plugin process.
    if not target:
      raise ValueError("No target specified.")

    start_port = int(start_port)
    end_port = int(end_port)

    if len(exceptions) > 0:
      exceptions = [
        int(x) for x in self.re.findall(r'\d+', exceptions)
        if x.isdigit()
    ]
    job_id = self.uuid(8)
    self.P(f"Launching {job_id=} {target=} with {exceptions=}")
    self.P(f"Announcing pentest to workers (instance_id {self.cfg_instance_id})...")
    job_specs = {
      "job_id" : job_id,
      "target": target,
      "exceptions" : exceptions,
      "start_port" : start_port,
      "end_port" : end_port,
      "launcher": self.ee_addr,
      "created_at": self.time(),
      "workers" : {
        self.ee_addr: {
          "finished": False,
          "result": None
        }
      },
    }
    self.chainstore_hset(
      hkey=self.cfg_instance_id,
      key=job_id,
      value=job_specs
    )
    all_network_jobs = self.chainstore_hgetall(hkey=self.cfg_instance_id)
    report = {}
    for other_key, other_spec in all_network_jobs.items():
      normalized_key, normalized_spec = self._normalize_job_record(other_key, other_spec)
      if normalized_key and normalized_key != job_id:
        report[normalized_key] = normalized_spec
    #end for
    
    self.P(f"Current jobs:\n{self.json_dumps(all_network_jobs, indent=2)}")
    result = {
      "job_specs": job_specs,
      "worker": self.ee_addr,
      "other_jobs": report,
    }
    return result


  @BasePlugin.endpoint
  def get_job_status(self, job_id: str):
    """
    Endpoint to retrieve the status or final report of a pentest job for the given target.
    
    TODO: Data must be extracted from CStore
    """
    # If job has completed, return its report
    return self._get_job_status(job_id)


  @BasePlugin.endpoint
  def list_network_jobs(self):
    """
    Endpoint to list all network jobs.
    """
    raw_network_jobs = self.chainstore_hgetall(hkey=self.cfg_instance_id)
    normalized_jobs = {}
    for job_key, job_spec in raw_network_jobs.items():
      normalized_key, normalized_spec = self._normalize_job_record(job_key, job_spec)
      if normalized_key and normalized_spec:
        normalized_jobs[normalized_key] = normalized_spec
    return normalized_jobs
  
  
  @BasePlugin.endpoint
  def list_local_jobs(self):
    jobs = {
      job_id: self._get_job_status(job_id)
      for job_id, local_workers in self.scan_jobs.items()
    }
    return jobs
  
  
  @BasePlugin.endpoint
  def stop_and_delete_job(self, job_id : str):
    """
    Endpoint to stop and delete a pentest job.
    """
    # Stop the job if it's running
    local_workers = self.scan_jobs.get(job_id)
    if local_workers:
      self.P(f"Stopping and deleting job {job_id}.")
      for local_worker_id, job in local_workers.items():
        self.P(f"Stopping job {job_id} on local worker {local_worker_id}.")
        job.stop()
      self.P(f"Job {job_id} stopped.")
    # Remove from active jobs
    self.scan_jobs.pop(job_id, None)
    raw_job_specs = self.chainstore_hget(hkey=self.cfg_instance_id, key=job_id)
    if isinstance(raw_job_specs, dict):
      _, job_specs = self._normalize_job_record(job_id, raw_job_specs)
      worker_entry = job_specs.setdefault("workers", {}).setdefault(self.ee_addr, {})
      worker_entry["finished"] = True
      worker_entry["canceled"] = True
      self.chainstore_hset(hkey=self.cfg_instance_id, key=job_id, value=job_specs)
    else:
      self.chainstore_hset(hkey=self.cfg_instance_id, key=job_id, value=None)
    self.P(f"Job {job_id} deleted.")
    return {"status": "success", "job_id": job_id}


  def process(self):
    """
    Periodically invoked to manage job threads.
    Launches new jobs and checks for completed ones.
    """
    super(PentesterApi01Plugin, self).process()

    if (self.time() - self.__warmupstart) < self.cfg_warmup_delay:
      # we do not start jobs before API warmup
      return
    elif not self.__warmup_done:
      self.__post_init()
    #endif 
    # Launch any new jobs
    self._maybe_launch_jobs()
    # Check active jobs for completion
    self._maybe_close_jobs()
    return
