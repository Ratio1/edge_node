
from naeural_core.business.default.web_app.fast_api_web_app import FastApiWebAppPlugin as BasePlugin
from .pentest_job_helper import PentestJobHelper  # Import PentestJob from separate module

__VER__ = '0.5.1'  # updated version

_CONFIG = {
  **BasePlugin.CONFIG,
  
  'PORT': None,
  
  "CHECK_JOBS_EACH" : 5,
  
  "WARMUP_DELAY" : 30,
  
  
  'VALIDATION_RULES': {
    **BasePlugin.CONFIG['VALIDATION_RULES'],  
  },
}

class PentesterApi01Plugin(BasePlugin):
  """
  RedMesh API - a pentesting meta-plugin for receiving pentesting targets and performing operations.
  Supports asynchronous job execution and performs distributed red-team attacks based on 
  decentralized workers orchestrated using CStore.
  """
  CONFIG = _CONFIG


  def on_init(self):
    super(PentesterApi01Plugin, self).on_init()
    self.__features = self._get_all_features()
    # Track active and completed jobs by target
    self.scan_jobs = {}       # target -> PentestJob instance
    self.completed_jobs_reports = {}  # target -> final report dict
    self.lst_completed_jobs = []  # List of completed jobs
    self.__last_checked_jobs = 0
    self.__warmupstart = self.time()
    self.__warmup_done = False
    current_epoch = self.netmon.epoch_manager.get_current_epoch()
    self.P("Started {} plugin in epoch {}. Current features:\n{}".format(
      self.__class__.__name__, current_epoch, self.__features
    ))
    return
  
  def P(self, s, *args, **kwargs):
    s = "[REDMESH] " + s
    return super(PentesterApi01Plugin, self).P(s, *args, **kwargs)
  
  
  def __post_init(self):
    all_network_jobs = self.chainstore_hgetall(hkey=self.cfg_instance_id)
    info = ""
    for target, job in all_network_jobs.items():
      if not isinstance(job, dict):
        info += f"- INVALID: {target}: {job}\n"
      else:
        info += f"- VALID: {target}: {self.json_dumps(job, indent=2)}\n"
    self.P("Warmup complete. Current jobs for {}:\n{}".format(
      self.cfg_instance_id, 
      info
    ))
    self.__warmup_done = True
    return



  def _get_all_features(self, categs=False):
    features = {} if categs else []
    PREFIXES = ["_service_info_", "_web_test_"]
    for prefix in PREFIXES:
      methods = [method for method in dir(PentestJobHelper) if method.startswith(prefix)]
      if categs:
        features[prefix[1:-1]] = methods
      else:
        features.extend(methods)
    return features


  def _maybe_launch_jobs(self, ports=None, exceptions=[64297], nr_batches=4):
    """
    Launch new PentestJob threads for any announced pentest target.
    Called at each process iteration.
    """
    if self.time() - self.__last_checked_jobs > self.cfg_check_jobs_each:      
      self.__last_checked_jobs = self.time()
      all_jobs = self.chainstore_hgetall(hkey=self.cfg_instance_id)
      for network_worker_address, job_specs in all_jobs.items():
        if not isinstance(job_specs, dict):
          continue
        target = job_specs.get("target")
        job_id = job_specs.get("job_id", target).replace(".", "")
        workers = job_specs.get("workers", {})
        current_worker_finished = workers.get(self.ee_addr, {}).get("finished", False)
        if job_id is None or current_worker_finished:
          continue  
        # If job not already running and not completed, start a new thread
        closed_target = job_id in self.completed_jobs_reports
        in_progress_target = job_id in self.scan_jobs
        if not in_progress_target and not closed_target:
          if ports is None:
            ports = list(range(65536))          
          batches = []
          ports = sorted(ports)
          nr_ports = len(ports)
          for i in range(nr_batches):
            start = i * (nr_ports // nr_batches)
            end = (i + 1) * (nr_ports // nr_batches)
            batch = ports[start:end]
            batches.append(batch)
          #endfor create batches
          if job_id not in self.scan_jobs:
            self.scan_jobs[job_id] = {}
          for batch in batches:
            self.P("Launching {} requested by {} for target {} - {} ports [{}-{}]".format(
              job_id, network_worker_address,
              target, len(batch), min(batch), max(batch)
            ))
            batch_job = PentestJobHelper(
              owner=self,
              target=target, 
              job_id=job_id,
              initiator=network_worker_address,
              exceptions=exceptions,
              worker_target_ports=batch,
            )
            self.scan_jobs[job_id][batch_job.local_worker_id] = batch_job
            batch_job.start()
        #endif
      #enfor
    #endif
    return
  
  
  def _close_job(self, job_id, report=None):
    """
    This only closes the LOCAL job not the whole network job - must be refactored!
    
    TODO: change the logic as follows
    - separate worker status from job status in different hset
      - redmesh instance (network) cfg_instance_id
      - job-id hset
    - each network worker will post status (for its local workers)
    - all network workers will monitor all network workers
    - last network worker that finishes posts 
    
    """
    local_jobs = self.scan_jobs.pop(job_id, None)
    if local_jobs:
      # TODO: Next line is WRONG - if the job was delivered from another network worker
      #       then we need to check the original worker that created the job
      job_specs = self.chainstore_hget(hkey=self.cfg_instance_id, key=self.ee_addr)
      forced_close = report is None
      closing = "Forced" if forced_close else "Post finish"
      self.P("{} closing job:\n{}\nWith report:\n{}".format(
        closing,
        self.json_dumps(job_specs, indent=2),
        self.json_dumps(report, indent=2)
      ))
      job_specs["workers"][self.ee_addr]["finished"] = True
      job_specs["workers"][self.ee_addr]["result"] = report
      job_specs["workers"][self.ee_addr]["forced_closed"] = forced_close
      self.chainstore_hset(hkey=self.cfg_instance_id, key=self.ee_addr, value=job_specs)
    return


  def _maybe_close_jobs(self):
    for job_id, local_workers in list(self.scan_jobs.items()):
      all_workers_done = True
      reports = {}
      job : PentestJobHelper = None
      initiator = None
      for local_worker_id, job in local_workers.items():
        # If thread finished or job flagged as done, collect result
        if not job.thread.is_alive() or job.state.get("done"):
          if job_id not in self.completed_jobs_reports:
            self.completed_jobs_reports[job_id] = {}
          # Prepare final report for this job
          local_worker_report = job.get_status()
          # Save completed report
          self.completed_jobs_reports[job_id][local_worker_id] = local_worker_report
          reports[local_worker_id] = local_worker_report
          self.P("Pentest job_id {} for target {} from local worker {} has finished:\n{}".format(
            job_id, local_worker_report['target'],
            local_worker_id,
            self.json_dumps(local_worker_report, indent=2)
          ))
        else:
          all_workers_done = False
        initiator = job.initiator
      #end for each worker
      
      if all_workers_done:
        self.lst_completed_jobs.append(job_id)
        self.P(f"All local workers for job {job_id} from initiator {initiator} have finished.")
        try:
          self._close_job(job_id, report=reports)
        except Exception as e:
          self.P(f"Error clearing job from chainstore: {e}")
          # Remove from active jobs
          del self.scan_jobs[job_id]
        #end if
      #end for each local worker of a job
    #end for each job
    return


  def _get_all_network_jobs(self):
    all_workers_and_jobs = self.chainstore_hgetall(hkey=self.cfg_instance_id)
    return all_workers_and_jobs


  def _get_job_from_cstore(self, job_id: str):
    all_workers_and_jobs = self._get_all_network_jobs()
    found = None
    for network_worker in all_workers_and_jobs:
      job_specs = all_workers_and_jobs.get(network_worker)
      if isinstance(job_specs, dict) and job_specs.get("job_id") == job_id:
        found = job_specs
        break
    return found


  def _get_job_status(self, job_id : str):
    target = None
    local_workers = self.scan_jobs.get(job_id)
    jobs_network_state = self._get_job_from_cstore(job_id)
    result = {}
    # first check if in completed jobs
    if job_id in self.lst_completed_jobs: 
      # dont check in the reports that might contain only from some local workers
      local_workers_reports = self.completed_jobs_reports[job_id]
      some_worker = list(local_workers_reports.keys())[0]
      target = local_workers_reports[some_worker]["target"]
      result = {
        "job_id": job_id,
        "target": target,
        "status": "completed",
        "report": self.completed_jobs_reports[job_id]
      }
    
    elif local_workers:
      # If job is currently running, return progress info   
      statuses = {}
      for local_worker_id, job in local_workers.items():
        statuses[local_worker_id] = job.get_status()
      #end for each local worker
      result = statuses
    # Job not found
    else:
      # TODO: check job in cstore maybe it was finished some time 
      #       ago before current deployment
      result = {
        "job_id": job_id,
        "target": target,
        "status": "not_found",
        "message": "No such job is running or has been completed on this worker."
      }
    #endif
    return result


  @BasePlugin.endpoint
  def list_features(self):
    result = {"features": self._get_all_features(categs=True)}
    return result


  @BasePlugin.endpoint
  def launch_test(self, target : str):
    """
    Endpoint to start a pentest on the specified target.
    Announce job to network via CStore and return current jobs.            
    """
    # INFO: This method only announces the job to the network. It does not 
    #       execute the job itself - that part is handled by PentestJob
    #       executed after periodical check from plugin process.
    job_id = self.uuid(8)
    self.P(f"Launching pentest {job_id} on target {target}")
    self.P(f"Announcing pentest to workers (instance_id {self.cfg_instance_id})...")
    job_specs = {
      "job_id" : job_id,
      "target": target,
      "workers" : {
        self.ee_addr: {
          "finished": False,
          "result": None
        }
      },
    }
    self.chainstore_hset(
      hkey=self.cfg_instance_id,
      key=self.ee_addr,
      value=job_specs
    )
    all_network_jobs = self.chainstore_hgetall(hkey=self.cfg_instance_id)
    report = {}
    for launcher, job_specs in all_network_jobs.items():
      is_current_job = isinstance(job_specs, dict) and job_specs.get("target") == target and launcher == self.ee_addr
      if not is_current_job:
        report[launcher] = job_specs
    #end for
    
    self.P(f"Current jobs:\n{self.json_dumps(all_network_jobs, indent=2)}")
    result = {
      "job_specs": job_specs,
      "worker": self.ee_addr,
      "other_jobs": report,
    }
    return result


  @BasePlugin.endpoint
  def get_job_status(self, job_id: str):
    """
    Endpoint to retrieve the status or final report of a pentest job for the given target.
    
    TODO: Data must be extracted from CStore
    """
    # If job has completed, return its report
    return self._get_job_status(job_id)


  @BasePlugin.endpoint
  def list_network_jobs(self):
    """
    Endpoint to list all network jobs.
    """
    all_network_jobs = self.chainstore_hgetall(hkey=self.cfg_instance_id)
    return all_network_jobs
  
  
  @BasePlugin.endpoint
  def list_local_jobs(self):
    jobs = {
      job_id: self._get_job_status(job_id)
      for job_id, local_workers in self.scan_jobs.items()
    }
    return jobs
  
  
  @BasePlugin.endpoint
  def stop_and_delete_job(self, job_id : str):
    """
    Endpoint to stop and delete a pentest job.
    """
    # Stop the job if it's running
    local_workers = self.scan_jobs.get(job_id)
    if local_workers:
      self.P(f"Stopping and deleting job {job_id}.")
      for local_worker_id, job in local_workers.items():
        self.P(f"Stopping job {job_id} on local worker {local_worker_id}.")
        job.stop()
      self.P(f"Job {job_id} stopped.")
    # Remove from active jobs
    self.scan_jobs.pop(job_id, None)
    self.P(f"Job {job_id} deleted.")
    return {"status": "success", "job_id": job_id}


  def process(self):
    """
    Periodically invoked to manage job threads.
    Launches new jobs and checks for completed ones.
    """
    super(PentesterApi01Plugin, self).process()
    
    if (self.time() - self.__warmupstart) < self.cfg_warmup_delay:
      # we do not start jobs before API warmup
      return
    elif not self.__warmup_done:
      self.__post_init()
    #endif 
    # Launch any new jobs
    self._maybe_launch_jobs()
    # Check active jobs for completion
    self._maybe_close_jobs()
    return